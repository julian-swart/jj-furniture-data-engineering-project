Meeting Minutes: 



Tues. March 15th , 2022 - 
- talked about problem definition, what we want to accomplish with this project, which is knowing and coding 
  the full flow of data ingestion from sources all the way to dashboarding and model building 
- the process as we see it: 
	- Raw data that needs to be ingested 
	- QA this raw data and send alerts to engineers and stakeholders 
	- Design Star Schema or other architecture type database 
	- ETL for analytics-ready data
	- Basetables for models
	- Create dashboards 
	- Build models 

Sun. March 20th, 2022 - 
- we decided that we wanted to mimic what a Customer Analytics team is doing for a corporation - need to come up with a fake company and fake data 
- attempted to create fake customer data in RStudio, wasn’t as easy as we hoped. Decided on trying Python next. 
- we first need to create a Transactional database, which mimics what companies have, then load that into a OLAP database


Tues. March 29th, 2022 - 
- set up GitHub repo
- started fake data creation - came up with fake furniture company “JJ Furniture” 
- tested push and pull requests, learned branching 


Thurs. March 31st, 2022 - 
- added Database folder for Table Setup python scripts and connections python script
- Jon walked me through how those work


Tues. April 5th, 2022 - 
- created Products table columns and added to table_setup.py 
- figured out how to import a script as a module from differing path levels using libraries “os” and “sys”


Tues. April 12th, 2022 - 
- created meeting minutes log
- Jon wanted to override local repo changes and git pull from the origin main, so he had to to a git reset —hard 
- Jon figured out better method for config.ini path - similar to how we are loading a python script as a module 
- still working on how to load our fake data into our transactional tables 
	- do we use pandas?
	- do we use a COPY command?
	- do we iterate through the rows of our fake data? 
	- found this blog which was helpful and validating: https://hakibenita.com/fast-load-data-python-postgresql 
	- Jon successfully added first test of 1,000 rows of customer data into postgres using the row-by-row method
	- we plan on following the above blog, we see that the COPY command is the fastest, but may take more memory
- I made changes and so did he, I want to pull request without deleting my changes, and then push my changes
	- I was on a previous branch, wanted to make a new branch, then commit, then pull from main to get Jon’s changes
	- git switch -c <new_branch>
	- git pull origin main
	- git commit 


Thurs. April 21st 
- from last meeting: 
	- TODO: learn best practices on GitHub for past branches (delete them?)
	- TODO: Julian to add product data to products table in next meeting
- we discovered through trying to make a product table that we need a Product table and a Product Sets table 
- also we needed to create the secondary/support tables first and then we'll sample from those to make the Product table 
- TODO: make SQL Connection code a function: 
- TODO: GitHub branch tutorial (didn't get to this today): https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-branches


Tues. May 3rd, 2022 -
- attempted to add material list to material table and insert it with an auto-incremented material_id using SERIAL. We saw that you can customize increments with SEQUENCE command in SQL. 
- ran into errors and attempted to resolve - figured it out! Way to go Jon. Needed tuples for the autogenerated SERIAL insert to work and needed one of the %s instead of two. Also had to restart the Python kernel when making a change to another script that was being read in. 
- What’s next: simulate transactions and inventory data
- then the fun begins! 


Thurs. May 26th, 2022
- realized we needed to simulate Product table first. Jon generated product data table with some sweet sampling code 
- what’s next: simulate transactions and possibly inventory data


Tues. May 31st, 2022
- started to simulate transaction by trying to solve getting different transaction_dates and weighting the years
- but we realized now we need to do it a different way - simulate 1 transaction at a time and insert it into the database 


Tues. June 7th, 2022
- successfully simulated transactions! Spent the 2 hour session coding up a nice function to do this. 
- next is to add in Returns and possibly inventory 
- then finally start doing some Data Engineering! 


Tues. June 28th, 2022 
- decided to do Returns next session and to not worry about inventory data at all 
- worked on setting up our OLAP database - first by researching what was out there - ultimately ended up going with Postgresql but using AWS to set it up 
- design star schema
- come up with business use cases to build tables for - i.e., predictive customer basetable, slow-changing dimension for new/repeat/returning/lapsed customers, analytic/dashboard ready data 

Thurs. July 7th, 2022
- designed Star Schema with draw.io
- learned a lot about SCD (Slow Changing Dimensions) and their Types (1,2,3,4)
- did not do Returns
- need to run the simulate transactions function to create more transactions 
- maybe create a analytics best-practices guide for how-to-use SQL queries (current_flag = 1, joins, the trans_date between effective_start date and effective_end_date)

Tues. July 12th, 2022
- made our database connection code into a function that gets imported as a module 

Tues. July 26th, 2022
- spent time trying to figure out how to go from oltp to olap (2 different databases, how do they connect?) and building our fact and dimension tables. Researched how to create the dim keys and how the fact keys are generated - we realized we create those for the end-user - they are solely join keys. We are shooting to be done by end-of-August - need a blog post to show off our efforts! 

Tues. August 16th, 2022
- had a guest appearance by Sumathi - a Data Engineer consultant that works with Julian at Vans. We had a few questions about the proper way to implement slow-changing dimensions (https://aws.amazon.com/blogs/big-data/implement-a-slowly-changing-dimension-in-amazon-redshift/) for a star schema and how to get data from our transactional database to our relational database within Redshift. She suggested we upload data to S3 first and then copy the data to our other database. Jon spent time figuring out how to add an IAM role and add me to it so we can do the upload command that we found for sql. Setting up IAM role was too complicated, which was needed in order to connect the RDS to the S3 bucket for the upload statement. We decided on going with python and boto3 instead. 











